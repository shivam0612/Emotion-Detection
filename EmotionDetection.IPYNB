{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPooling2D, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "images=[]\n",
    "targets=[]\n",
    "\n",
    "angry= r'D:\\Shivam\\Projects\\ML & AI\\Emotion Detection\\Dataset\\angry'\n",
    "happy=r'D:\\Shivam\\Projects\\ML & AI\\Emotion Detection\\Dataset\\happy'\n",
    "sad=r'D:\\Shivam\\Projects\\ML & AI\\Emotion Detection\\Dataset\\sad'\n",
    "neutral=r'D:\\Shivam\\Projects\\ML & AI\\Emotion Detection\\Dataset\\neutral'\n",
    "surprise=r'D:\\Shivam\\Projects\\ML & AI\\Emotion Detection\\Dataset\\surprise'\n",
    "\n",
    "# angry : 1, happy : 2, sad : 3,neutral : 4, sueprise : 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "content=os.listdir(angry)\n",
    "\n",
    "for image in content:\n",
    "    try:\n",
    "        image_path=angry + '\\\\'+ image\n",
    "        image=cv2.imread(image_path)\n",
    "        image_grey=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "        resized_image=cv2.resize(image_grey,(100,100))\n",
    "        images.append(resized_image)\n",
    "        targets.append(1)\n",
    "    except Exception as e:\n",
    "        print(\"exception\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "content=os.listdir(happy)\n",
    "\n",
    "for image in content:\n",
    "    try:\n",
    "        image_path=happy + '\\\\'+ image\n",
    "        image=cv2.imread(image_path)\n",
    "        image_grey=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "        resized_image=cv2.resize(image_grey,(100,100))\n",
    "        images.append(resized_image)\n",
    "        targets.append(2)\n",
    "    except Exception as e:\n",
    "        print(\"exception\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "content=os.listdir(sad)\n",
    "\n",
    "for image in content:\n",
    "    try:\n",
    "        image_path=sad + '\\\\'+ image\n",
    "        image=cv2.imread(image_path)\n",
    "        image_grey=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "        resized_image=cv2.resize(image_grey,(100,100))\n",
    "        images.append(resized_image)\n",
    "        targets.append(3)\n",
    "    except Exception as e:\n",
    "        print(\"exception\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "content=os.listdir(neutral)\n",
    "\n",
    "for image in content:\n",
    "    try:\n",
    "        image_path=neutral + '\\\\'+ image\n",
    "        image=cv2.imread(image_path)\n",
    "        image_grey=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "        resized_image=cv2.resize(image_grey,(100,100))\n",
    "        images.append(resized_image)\n",
    "        targets.append(4)\n",
    "    except Exception as e:\n",
    "        print(\"exception\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "content=os.listdir(surprise)\n",
    "\n",
    "for image in content:\n",
    "    try:\n",
    "        image_path=surprise + '\\\\'+ image\n",
    "        image=cv2.imread(image_path)\n",
    "        image_grey=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "        resized_image=cv2.resize(image_grey,(100,100))\n",
    "        images.append(resized_image)\n",
    "        targets.append(5)\n",
    "    except Exception as e:\n",
    "        print(\"exception\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array(images)/255.0\n",
    "targets = np.array(targets)/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(images, targets, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "X_train.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.reshape(X_train.shape[0],100, 100, 1)\n",
    "X_test=X_test.reshape(X_test.shape[0],100,100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "Y_train = label_encoder.fit_transform(Y_train)\n",
    "Y_test=label_encoder.fit_transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train=np_utils.to_categorical(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Sequential()\n",
    "\n",
    "model.add(Conv2D(200, (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(150, (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Dropout((0.4)))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(50,activation='relu'))\n",
    "model.add(Dense(5,activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/14\n",
      "529/529 [==============================] - 1516s 3s/step - loss: 1.5481 - accuracy: 0.3058 - val_loss: 1.3684 - val_accuracy: 0.4354\n",
      "INFO:tensorflow:Assets written to: model-best\\assets\n",
      "Epoch 2/14\n",
      "529/529 [==============================] - 1460s 3s/step - loss: 1.3447 - accuracy: 0.4430 - val_loss: 1.3175 - val_accuracy: 0.4630\n",
      "INFO:tensorflow:Assets written to: model-best\\assets\n",
      "Epoch 3/14\n",
      "529/529 [==============================] - 1510s 3s/step - loss: 1.2686 - accuracy: 0.4790 - val_loss: 1.2607 - val_accuracy: 0.4845\n",
      "INFO:tensorflow:Assets written to: model-best\\assets\n",
      "Epoch 4/14\n",
      "529/529 [==============================] - 1511s 3s/step - loss: 1.2066 - accuracy: 0.5087 - val_loss: 1.2346 - val_accuracy: 0.4954\n",
      "INFO:tensorflow:Assets written to: model-best\\assets\n",
      "Epoch 5/14\n",
      "529/529 [==============================] - 1514s 3s/step - loss: 1.1648 - accuracy: 0.5211 - val_loss: 1.2390 - val_accuracy: 0.5025\n",
      "Epoch 6/14\n",
      "529/529 [==============================] - 1512s 3s/step - loss: 1.1163 - accuracy: 0.5433 - val_loss: 1.1878 - val_accuracy: 0.5115\n",
      "INFO:tensorflow:Assets written to: model-best\\assets\n",
      "Epoch 7/14\n",
      "529/529 [==============================] - 1514s 3s/step - loss: 1.0742 - accuracy: 0.5588 - val_loss: 1.1691 - val_accuracy: 0.5197\n",
      "INFO:tensorflow:Assets written to: model-best\\assets\n",
      "Epoch 8/14\n",
      "529/529 [==============================] - 1459s 3s/step - loss: 1.0453 - accuracy: 0.5754 - val_loss: 1.1837 - val_accuracy: 0.5301\n",
      "Epoch 9/14\n",
      "529/529 [==============================] - 1427s 3s/step - loss: 1.0148 - accuracy: 0.5832 - val_loss: 1.1642 - val_accuracy: 0.5327\n",
      "INFO:tensorflow:Assets written to: model-best\\assets\n",
      "Epoch 10/14\n",
      "529/529 [==============================] - 1428s 3s/step - loss: 0.9873 - accuracy: 0.6069 - val_loss: 1.1567 - val_accuracy: 0.5304\n",
      "INFO:tensorflow:Assets written to: model-best\\assets\n",
      "Epoch 11/14\n",
      "529/529 [==============================] - 1517s 3s/step - loss: 0.9516 - accuracy: 0.6208 - val_loss: 1.1543 - val_accuracy: 0.5389\n",
      "INFO:tensorflow:Assets written to: model-best\\assets\n",
      "Epoch 12/14\n",
      "529/529 [==============================] - 1525s 3s/step - loss: 0.9433 - accuracy: 0.6206 - val_loss: 1.1637 - val_accuracy: 0.5384\n",
      "Epoch 13/14\n",
      "529/529 [==============================] - 1523s 3s/step - loss: 0.9098 - accuracy: 0.6392 - val_loss: 1.1666 - val_accuracy: 0.5431\n",
      "Epoch 14/14\n",
      "529/529 [==============================] - 1524s 3s/step - loss: 0.8833 - accuracy: 0.6415 - val_loss: 1.1643 - val_accuracy: 0.5403\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b548d8a040>"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "cp = ModelCheckpoint('model-best', verbose=0, save_best_only=True)\n",
    "model.fit(X_train, Y_train, epochs = 14,  callbacks=[cp], validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}