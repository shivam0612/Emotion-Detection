{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPooling2D, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Variables and Setting path\n",
    "images=[]\n",
    "targets=[]\n",
    "\n",
    "# {angry : 0, happy : 1, sad : 2,neutral : 3, surprise : 4 } - 5 Diffetent EMOTIONS\n",
    "angry= r'D:\\Shivam\\Projects\\ML & AI\\Emotion Detection\\Dataset\\angry'\n",
    "happy=r'D:\\Shivam\\Projects\\ML & AI\\Emotion Detection\\Dataset\\happy'\n",
    "sad=r'D:\\Shivam\\Projects\\ML & AI\\Emotion Detection\\Dataset\\sad'\n",
    "neutral=r'D:\\Shivam\\Projects\\ML & AI\\Emotion Detection\\Dataset\\neutral'\n",
    "surprise=r'D:\\Shivam\\Projects\\ML & AI\\Emotion Detection\\Dataset\\surprise'\n",
    "fear=r'D:\\Shivam\\Projects\\ML & AI\\Emotion Detection\\Dataset\\fear'\n",
    "disgust=r'D:\\Shivam\\Projects\\ML & AI\\Emotion Detection\\Dataset\\disgust'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Getting Image, Resizing, Converting It To Gray Scale, Storing in List Variables\n",
    "content=os.listdir(angry)\n",
    "\n",
    "for image in content:\n",
    "    try:\n",
    "        image_path=angry + '\\\\'+ image\n",
    "        image=cv2.imread(image_path)\n",
    "        image_grey=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "        resized_image=cv2.resize(image_grey,(48,48))\n",
    "        images.append(resized_image)\n",
    "        targets.append(0)\n",
    "    except Exception as e:\n",
    "        print(\"exception\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Getting Image, Resizing, Converting It To Gray Scale, Storing in List Variables\n",
    "content=os.listdir(happy)\n",
    "\n",
    "for image in content:\n",
    "    try:\n",
    "        image_path=happy + '\\\\'+ image\n",
    "        image=cv2.imread(image_path)\n",
    "        image_grey=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "        resized_image=cv2.resize(image_grey,(48,48))\n",
    "        images.append(resized_image)\n",
    "        targets.append(1)\n",
    "    except Exception as e:\n",
    "        print(\"exception\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Image, Resizing, Converting It To Gray Scale, Storing in List Variables\n",
    "content=os.listdir(sad)\n",
    "\n",
    "for image in content:\n",
    "    try:\n",
    "        image_path=sad + '\\\\'+ image\n",
    "        image=cv2.imread(image_path)\n",
    "        image_grey=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "        resized_image=cv2.resize(image_grey,(48,48))\n",
    "        images.append(resized_image)\n",
    "        targets.append(2)\n",
    "    except Exception as e:\n",
    "        print(\"exception\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Image, Resizing, Converting It To Gray Scale, Storing in List Variables\n",
    "content=os.listdir(neutral)\n",
    "\n",
    "for image in content:\n",
    "    try:\n",
    "        image_path=neutral + '\\\\'+ image\n",
    "        image=cv2.imread(image_path)\n",
    "        image_grey=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "        resized_image=cv2.resize(image_grey,(48,48))\n",
    "        images.append(resized_image)\n",
    "        targets.append(3)\n",
    "    except Exception as e:\n",
    "        print(\"exception\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Image, Resizing, Converting It To Gray Scale, Storing in List Variables\n",
    "content=os.listdir(surprise)\n",
    "\n",
    "for image in content:\n",
    "    try:\n",
    "        image_path=surprise + '\\\\'+ image\n",
    "        image=cv2.imread(image_path)\n",
    "        image_grey=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "        resized_image=cv2.resize(image_grey,(48,48))\n",
    "        images.append(resized_image)\n",
    "        targets.append(4)\n",
    "    except Exception as e:\n",
    "        print(\"exception\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Image, Resizing, Converting It To Gray Scale, Storing in List Variables\n",
    "content=os.listdir(fear)\n",
    "\n",
    "for image in content:\n",
    "    try:\n",
    "        image_path=fear + '\\\\'+ image\n",
    "        image=cv2.imread(image_path)\n",
    "        image_grey=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "        resized_image=cv2.resize(image_grey,(48,48))\n",
    "        images.append(resized_image)\n",
    "        targets.append(5)\n",
    "    except Exception as e:\n",
    "        print(\"exception\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Image, Resizing, Converting It To Gray Scale, Storing in List Variables\n",
    "content=os.listdir(disgust)\n",
    "\n",
    "for image in content:\n",
    "    try:\n",
    "        image_path=disgust + '\\\\'+ image\n",
    "        image=cv2.imread(image_path)\n",
    "        image_grey=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "        resized_image=cv2.resize(image_grey,(48,48))\n",
    "        images.append(resized_image)\n",
    "        targets.append(6)\n",
    "    except Exception as se:\n",
    "        print(\"exception\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "images = np.array(images)/255.0\n",
    "targets = np.array(targets)/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining list variable for training \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(images, targets, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# Checking Array Dimensions\n",
    "X_train.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping\n",
    "X_train=X_train.reshape(X_train.shape[0],48, 48, 1)\n",
    "X_test=X_test.reshape(X_test.shape[0],48,48,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding For Handling Categorical Values\n",
    "label_encoder = LabelEncoder()\n",
    "Y_train = label_encoder.fit_transform(Y_train)\n",
    "Y_test=label_encoder.fit_transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting To Binary Class Matrix\n",
    "Y_train=np_utils.to_categorical(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Sequential model and Layers\n",
    "model= Sequential()\n",
    "\n",
    "model.add(Conv2D(200, (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(150, (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(100, (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Dropout((0.4)))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(100,activation='relu'))\n",
    "model.add(Dense(7,activation='softmax'))\n",
    "\n",
    "# Compiling Model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "628/628 [==============================] - 420s 666ms/step - loss: 1.8161 - accuracy: 0.2501 - val_loss: 1.6282 - val_accuracy: 0.3457\n",
      "INFO:tensorflow:Assets written to: model-ED\\assets\n",
      "Epoch 2/50\n",
      "628/628 [==============================] - 443s 705ms/step - loss: 1.5837 - accuracy: 0.3767 - val_loss: 1.4775 - val_accuracy: 0.4262\n",
      "INFO:tensorflow:Assets written to: model-ED\\assets\n",
      "Epoch 3/50\n",
      "628/628 [==============================] - 505s 804ms/step - loss: 1.4406 - accuracy: 0.4444 - val_loss: 1.3612 - val_accuracy: 0.4705\n",
      "INFO:tensorflow:Assets written to: model-ED\\assets\n",
      "Epoch 4/50\n",
      "628/628 [==============================] - 462s 736ms/step - loss: 1.3612 - accuracy: 0.4750 - val_loss: 1.3298 - val_accuracy: 0.4891\n",
      "INFO:tensorflow:Assets written to: model-ED\\assets\n",
      "Epoch 5/50\n",
      "628/628 [==============================] - 408s 649ms/step - loss: 1.2966 - accuracy: 0.5028 - val_loss: 1.3136 - val_accuracy: 0.5060\n",
      "INFO:tensorflow:Assets written to: model-ED\\assets\n",
      "Epoch 6/50\n",
      "628/628 [==============================] - 405s 645ms/step - loss: 1.2456 - accuracy: 0.5208 - val_loss: 1.2463 - val_accuracy: 0.5183\n",
      "INFO:tensorflow:Assets written to: model-ED\\assets\n",
      "Epoch 7/50\n",
      "628/628 [==============================] - 402s 640ms/step - loss: 1.1994 - accuracy: 0.5413 - val_loss: 1.2424 - val_accuracy: 0.5213\n",
      "INFO:tensorflow:Assets written to: model-ED\\assets\n",
      "Epoch 8/50\n",
      "628/628 [==============================] - 404s 644ms/step - loss: 1.1798 - accuracy: 0.5517 - val_loss: 1.2395 - val_accuracy: 0.5269\n",
      "INFO:tensorflow:Assets written to: model-ED\\assets\n",
      "Epoch 9/50\n",
      "628/628 [==============================] - 403s 641ms/step - loss: 1.1340 - accuracy: 0.5688 - val_loss: 1.2021 - val_accuracy: 0.5422\n",
      "INFO:tensorflow:Assets written to: model-ED\\assets\n",
      "Epoch 10/50\n",
      "628/628 [==============================] - 403s 642ms/step - loss: 1.1027 - accuracy: 0.5817 - val_loss: 1.2121 - val_accuracy: 0.5402\n",
      "Epoch 11/50\n",
      "628/628 [==============================] - 404s 643ms/step - loss: 1.0785 - accuracy: 0.5808 - val_loss: 1.2056 - val_accuracy: 0.5472\n",
      "Epoch 12/50\n",
      "628/628 [==============================] - 404s 643ms/step - loss: 1.0435 - accuracy: 0.6075 - val_loss: 1.1814 - val_accuracy: 0.5539\n",
      "INFO:tensorflow:Assets written to: model-ED\\assets\n",
      "Epoch 13/50\n",
      "628/628 [==============================] - 403s 642ms/step - loss: 1.0112 - accuracy: 0.6168 - val_loss: 1.1837 - val_accuracy: 0.5567\n",
      "Epoch 14/50\n",
      "628/628 [==============================] - 402s 640ms/step - loss: 0.9969 - accuracy: 0.6260 - val_loss: 1.1954 - val_accuracy: 0.5464\n",
      "Epoch 15/50\n",
      "628/628 [==============================] - 402s 641ms/step - loss: 0.9668 - accuracy: 0.6352 - val_loss: 1.1966 - val_accuracy: 0.5545\n",
      "Epoch 16/50\n",
      "628/628 [==============================] - 402s 641ms/step - loss: 0.9558 - accuracy: 0.6431 - val_loss: 1.1914 - val_accuracy: 0.5523\n",
      "Epoch 17/50\n",
      "628/628 [==============================] - 403s 641ms/step - loss: 0.9358 - accuracy: 0.6483 - val_loss: 1.1927 - val_accuracy: 0.5607\n",
      "Epoch 18/50\n",
      "628/628 [==============================] - 402s 640ms/step - loss: 0.9011 - accuracy: 0.6681 - val_loss: 1.2149 - val_accuracy: 0.5533\n",
      "Epoch 19/50\n",
      "628/628 [==============================] - 398s 634ms/step - loss: 0.8872 - accuracy: 0.6668 - val_loss: 1.1907 - val_accuracy: 0.5529\n",
      "Epoch 20/50\n",
      "628/628 [==============================] - 398s 635ms/step - loss: 0.8688 - accuracy: 0.6717 - val_loss: 1.2186 - val_accuracy: 0.5611\n",
      "Epoch 21/50\n",
      "628/628 [==============================] - 397s 633ms/step - loss: 0.8464 - accuracy: 0.6805 - val_loss: 1.2317 - val_accuracy: 0.5500\n",
      "Epoch 22/50\n",
      "628/628 [==============================] - 398s 633ms/step - loss: 0.8447 - accuracy: 0.6834 - val_loss: 1.2169 - val_accuracy: 0.5543\n",
      "Epoch 23/50\n",
      "628/628 [==============================] - 398s 633ms/step - loss: 0.8155 - accuracy: 0.6930 - val_loss: 1.2082 - val_accuracy: 0.5605\n",
      "Epoch 24/50\n",
      "628/628 [==============================] - 398s 633ms/step - loss: 0.8134 - accuracy: 0.6922 - val_loss: 1.2283 - val_accuracy: 0.5492\n",
      "Epoch 25/50\n",
      "628/628 [==============================] - 399s 635ms/step - loss: 0.7933 - accuracy: 0.7026 - val_loss: 1.2228 - val_accuracy: 0.5565\n",
      "Epoch 26/50\n",
      "628/628 [==============================] - 397s 632ms/step - loss: 0.7803 - accuracy: 0.7005 - val_loss: 1.2450 - val_accuracy: 0.5575\n",
      "Epoch 27/50\n",
      "628/628 [==============================] - 399s 635ms/step - loss: 0.7453 - accuracy: 0.7185 - val_loss: 1.2510 - val_accuracy: 0.5523\n",
      "Epoch 28/50\n",
      "628/628 [==============================] - 398s 633ms/step - loss: 0.7407 - accuracy: 0.7260 - val_loss: 1.2656 - val_accuracy: 0.5607\n",
      "Epoch 29/50\n",
      "628/628 [==============================] - 397s 633ms/step - loss: 0.7186 - accuracy: 0.7270 - val_loss: 1.2958 - val_accuracy: 0.5601\n",
      "Epoch 30/50\n",
      "628/628 [==============================] - 401s 639ms/step - loss: 0.7026 - accuracy: 0.7436 - val_loss: 1.2924 - val_accuracy: 0.5529\n",
      "Epoch 31/50\n",
      "628/628 [==============================] - 398s 633ms/step - loss: 0.7043 - accuracy: 0.7326 - val_loss: 1.3156 - val_accuracy: 0.5585\n",
      "Epoch 32/50\n",
      "628/628 [==============================] - 398s 634ms/step - loss: 0.6939 - accuracy: 0.7405 - val_loss: 1.3241 - val_accuracy: 0.5500\n",
      "Epoch 33/50\n",
      "628/628 [==============================] - 398s 633ms/step - loss: 0.6731 - accuracy: 0.7501 - val_loss: 1.3507 - val_accuracy: 0.5468\n",
      "Epoch 34/50\n",
      "628/628 [==============================] - 398s 634ms/step - loss: 0.6688 - accuracy: 0.7538 - val_loss: 1.3442 - val_accuracy: 0.5539\n",
      "Epoch 35/50\n",
      "628/628 [==============================] - 404s 644ms/step - loss: 0.6538 - accuracy: 0.7551 - val_loss: 1.3609 - val_accuracy: 0.5472\n",
      "Epoch 36/50\n",
      "628/628 [==============================] - 402s 639ms/step - loss: 0.6463 - accuracy: 0.7604 - val_loss: 1.3674 - val_accuracy: 0.5521\n",
      "Epoch 37/50\n",
      "628/628 [==============================] - 403s 642ms/step - loss: 0.6479 - accuracy: 0.7565 - val_loss: 1.3713 - val_accuracy: 0.5484\n",
      "Epoch 38/50\n",
      "628/628 [==============================] - 403s 642ms/step - loss: 0.6223 - accuracy: 0.7669 - val_loss: 1.4079 - val_accuracy: 0.5514\n",
      "Epoch 39/50\n",
      "628/628 [==============================] - 403s 642ms/step - loss: 0.6130 - accuracy: 0.7687 - val_loss: 1.3979 - val_accuracy: 0.5533\n",
      "Epoch 40/50\n",
      "628/628 [==============================] - 403s 642ms/step - loss: 0.6105 - accuracy: 0.7747 - val_loss: 1.4084 - val_accuracy: 0.5504\n",
      "Epoch 41/50\n",
      "628/628 [==============================] - 404s 643ms/step - loss: 0.5962 - accuracy: 0.7795 - val_loss: 1.4159 - val_accuracy: 0.5510\n",
      "Epoch 42/50\n",
      "628/628 [==============================] - 404s 644ms/step - loss: 0.5907 - accuracy: 0.7807 - val_loss: 1.4368 - val_accuracy: 0.5520\n",
      "Epoch 43/50\n",
      "628/628 [==============================] - 405s 644ms/step - loss: 0.5621 - accuracy: 0.7947 - val_loss: 1.4876 - val_accuracy: 0.5428\n",
      "Epoch 44/50\n",
      "628/628 [==============================] - 403s 642ms/step - loss: 0.5895 - accuracy: 0.7785 - val_loss: 1.4261 - val_accuracy: 0.5454\n",
      "Epoch 45/50\n",
      "628/628 [==============================] - 405s 646ms/step - loss: 0.5699 - accuracy: 0.7939 - val_loss: 1.4387 - val_accuracy: 0.5555\n",
      "Epoch 46/50\n",
      "628/628 [==============================] - 403s 641ms/step - loss: 0.5469 - accuracy: 0.7988 - val_loss: 1.4920 - val_accuracy: 0.5468\n",
      "Epoch 47/50\n",
      "628/628 [==============================] - 403s 642ms/step - loss: 0.5508 - accuracy: 0.7940 - val_loss: 1.4383 - val_accuracy: 0.5488\n",
      "Epoch 48/50\n",
      "628/628 [==============================] - 399s 635ms/step - loss: 0.5329 - accuracy: 0.8016 - val_loss: 1.4897 - val_accuracy: 0.5484\n",
      "Epoch 49/50\n",
      "628/628 [==============================] - 399s 636ms/step - loss: 0.5422 - accuracy: 0.8016 - val_loss: 1.4653 - val_accuracy: 0.5488\n",
      "Epoch 50/50\n",
      "628/628 [==============================] - 522s 831ms/step - loss: 0.5361 - accuracy: 0.8065 - val_loss: 1.4746 - val_accuracy: 0.5476\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2aeb8013370>"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "# Saving Model For Future Use\n",
    "cp = ModelCheckpoint('model-ED', verbose=0, save_best_only=True)\n",
    "# Training Model\n",
    "model.fit(X_train, Y_train, epochs = 50,  callbacks=[cp], validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries For Loading Model\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Model\n",
    "model=load_model('model-best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Face Detection\n",
    "face_detect=cv2.CascadeClassifier(r'haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capturing Video\n",
    "source = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    not_to_use, image = source.read()\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_detect.detectMultiScale(gray, 1.5,5)\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_roi = gray[y:y+w, x:x+w]\n",
    "        resized_face = cv2.resize(face_roi, (100, 100))\n",
    "        normalized_Face = resized_face/255\n",
    "        reshaped_face = np.reshape(normalized_Face, (1, 100, 100, 1))\n",
    "        result = model.predict(reshaped_face)[0]\n",
    "        \n",
    "        # Using 5 Emotions as per definition        \n",
    "       \n",
    "        if np.amax(result)==result[0]:\n",
    "            cv2.rectangle(image,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "            cv2.putText(image,\"Angry\",(x,y),cv2.FONT_HERSHEY_SIMPLEX,fontScale=1,color=(255,0,0),thickness=2)\n",
    "        if np.amax(result)==result[1]:\n",
    "            cv2.rectangle(image,(x,y),(x+w,y+h),(255,0,255),2)\n",
    "            cv2.putText(image,\"happy\",(x,y),cv2.FONT_HERSHEY_SIMPLEX,fontScale=1,color=(255,0,255),thickness=2)\n",
    "        if np.amax(result)==result[2]:\n",
    "            cv2.rectangle(image,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "            cv2.putText(image,\"neutral\",(x,y),cv2.FONT_HERSHEY_SIMPLEX,fontScale=1,color=(0,255,0),thickness=2)\n",
    "        if np.amax(result)==result[3]:\n",
    "            cv2.rectangle(image,(x,y),(x+w,y+h),(255,255,50),2)\n",
    "            cv2.putText(image,\"sad\",(x,y),cv2.FONT_HERSHEY_SIMPLEX,fontScale=1,color=(255,255,50),thickness=2)\n",
    "        if np.amax(result)==result[4]:\n",
    "            cv2.rectangle(image,(x,y),(x+w,y+h),(17,17,17),2)\n",
    "            cv2.putText(image,\"surprised\",(x,y),cv2.FONT_HERSHEY_SIMPLEX,fontScale=1,color=(17,17,17),thickness=2)\n",
    "\n",
    "    Height=600\n",
    "    Width=600\n",
    "    dimension = (Width, Height)\n",
    "    resized_image = cv2.resize(image, dimension, interpolation = cv2.INTER_AREA)\n",
    "\n",
    "    cv2.imshow('Verzeo Final Project', resized_image)\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27:\n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "source.release()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}